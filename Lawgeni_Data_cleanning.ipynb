{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN64GYP7W6bBkC0YMrNab3t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shobhit115/HackIndia-Spark-9-2025-Code-Hackers/blob/main/Lawgeni_Data_cleanning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjJgTe3jZfs7",
        "outputId": "f73e1b46-89e2-4d28-96ad-2fa03db75da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged 5530 unique links saved to 'merged_links.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load both CSV files\n",
        "csv1 = pd.read_csv('/content/direct_pdfs_found.csv', header=None)\n",
        "csv2 = pd.read_csv('/content/scraped_pdfs_found.csv', header=None)\n",
        "\n",
        "# Extract link columns (first col in csv1, second col in csv2)\n",
        "links1 = csv1.iloc[:, 0]\n",
        "links2 = csv2.iloc[:, 1]\n",
        "\n",
        "# Combine all links into one list\n",
        "merged_links = pd.concat([links1, links2], ignore_index=True)\n",
        "\n",
        "# Remove duplicates and empty values\n",
        "merged_links = merged_links.dropna().drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Save to new CSV\n",
        "merged_links.to_csv('merged_links.csv', index=False, header=['pdf_link'])\n",
        "\n",
        "print(f\"Merged {len(merged_links)} unique links saved to 'merged_links.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "954UvnknePH4",
        "outputId": "e105edc6-b03b-4703-b7f4-77e8e1e45d30"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(path):\n",
        "    text = \"\"\n",
        "    with fitz.open(path) as doc:\n",
        "        for page in doc:\n",
        "            text += page.get_text(\"text\") + \"\\n\"\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "qhN-tnNHb-p2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTKRdSJ1eWXZ",
        "outputId": "711ac06b-7fde-41ea-b254-9fea55f8fe15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def extract_text_from_scanned_pdf(path):\n",
        "    text = \"\"\n",
        "    with fitz.open(path) as doc:\n",
        "        for page in doc:\n",
        "            pix = page.get_pixmap()\n",
        "            img = Image.open(io.BytesIO(pix.tobytes()))\n",
        "            text += pytesseract.image_to_string(img)\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "snL9ny5ab_im"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smart_extract(path):\n",
        "    text = extract_text_from_pdf(path)\n",
        "    if len(text.strip()) < 50:  # likely scanned\n",
        "        text = extract_text_from_scanned_pdf(path)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "03JdBEpDcCjt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def structure_legal_text(text):\n",
        "    # Normalize spaces\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # --- Title Extraction ---\n",
        "    title_pattern = re.compile(\n",
        "        r'(?:THE\\s+[A-Z\\s\\(\\)\\-]+ACT(?:,?\\s*\\d{4})?|ACT\\s+NO\\.?\\s*[A-Z0-9\\-]*\\.?\\s*OF\\s*\\d{4})',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    title_match = title_pattern.search(cleaned_text)\n",
        "    title = title_match.group(0).strip() if title_match else None\n",
        "\n",
        "    # --- Act Year Extraction ---\n",
        "    year_match = re.search(r'(?:OF|,)\\s*(\\d{4})', cleaned_text, re.IGNORECASE)\n",
        "    act_year = int(year_match.group(1)) if year_match else None\n",
        "\n",
        "    # --- Section / Clause Detection ---\n",
        "\n",
        "    sections = re.findall(\n",
        "        r'\\b(?:Section\\s+\\d+[A-Za-z]?|\\b\\d{1,3}\\.\\s+[A-Z]|\\b[IVXLCDM]{1,6}\\.)',\n",
        "        cleaned_text\n",
        "    )\n",
        "\n",
        "    structured = {\n",
        "        \"title\": title,\n",
        "        \"act_year\": act_year,\n",
        "        \"section_count\": len(sections),\n",
        "        \"text\": cleaned_text\n",
        "    }\n",
        "\n",
        "    return structured\n"
      ],
      "metadata": {
        "id": "iiKU1ucTcHdH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing purpose\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# ---- Import your previous functions ----\n",
        "# from your_module import smart_extract, structure_legal_text\n",
        "\n",
        "# Load the merged CSV containing PDF links\n",
        "df = pd.read_csv('merged_links.csv')\n",
        "pdf_links = df['pdf_link']\n",
        "\n",
        "# Take first 10 for testing\n",
        "test_links = pdf_links.head(10)\n",
        "\n",
        "# Create folders\n",
        "pdf_dir = Path(\"pdfs\")\n",
        "pdf_dir.mkdir(exist_ok=True)\n",
        "\n",
        "output_dir = Path(\"outputs\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Storage for structured output\n",
        "output = []\n",
        "\n",
        "# ---- Process each PDF ----\n",
        "for i, link in enumerate(tqdm(test_links, desc=\"Processing PDFs\", unit=\"pdf\")):\n",
        "    pdf_name = Path(link).name\n",
        "    pdf_path = pdf_dir / pdf_name\n",
        "\n",
        "    # Step 1: Download PDF if not already present\n",
        "    if not pdf_path.exists():\n",
        "        os.system(f\"curl -L -o '{pdf_path}' '{link}'\")\n",
        "\n",
        "    # Step 2: Extract text (auto-detects scanned vs normal)\n",
        "    try:\n",
        "        text = smart_extract(pdf_path)\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] {pdf_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Step 3: Structure the extracted text\n",
        "    data = structure_legal_text(text)\n",
        "    data[\"file_name\"] = pdf_name\n",
        "    data[\"pdf_link\"] = link  # ✅ Add the source PDF link\n",
        "\n",
        "    output.append(data)\n",
        "\n",
        "# ---- Save results ----\n",
        "output_df = pd.DataFrame(output)\n",
        "output_path = output_dir / 'structured_output_test.csv'\n",
        "output_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\n✅ Processed {len(output)} PDFs successfully.\")\n",
        "print(f\"Output saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLAE1OJncPj_",
        "outputId": "b26ee7eb-f3c9-4513-d25b-9e0038173ff6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PDFs:  30%|███       | 3/10 [00:00<00:00, 19.87pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Error] direct_pdf_link: no such file: 'pdfs/direct_pdf_link'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PDFs: 100%|██████████| 10/10 [00:00<00:00, 22.62pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Processed 9 PDFs successfully.\n",
            "Output saved to outputs/structured_output_test.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing purpose\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"outputs/structured_output_test.csv\")\n",
        "df.head(10).to_json(\"outputs/preview.json\", orient=\"records\", indent=4)\n"
      ],
      "metadata": {
        "id": "k4uGwZDxfuNl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import math\n",
        "\n",
        "# ---- Import your helper functions ----\n",
        "# from your_module import smart_extract, structure_legal_text\n",
        "\n",
        "# Load merged CSV\n",
        "df = pd.read_csv('merged_links.csv')\n",
        "pdf_links = df['pdf_link'].dropna().tolist()\n",
        "\n",
        "# Create folders\n",
        "pdf_dir = Path(\"pdfs\")\n",
        "pdf_dir.mkdir(exist_ok=True)\n",
        "\n",
        "output_dir = Path(\"outputs\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 100\n",
        "num_batches = math.ceil(len(pdf_links) / BATCH_SIZE)\n",
        "\n",
        "print(f\"Total PDFs: {len(pdf_links)} → {num_batches} batches of {BATCH_SIZE} each\\n\")\n",
        "\n",
        "# ---- Process in batches ----\n",
        "for batch_num in range(num_batches):\n",
        "    start_idx = batch_num * BATCH_SIZE\n",
        "    end_idx = min(start_idx + BATCH_SIZE, len(pdf_links))\n",
        "    batch_links = pdf_links[start_idx:end_idx]\n",
        "\n",
        "    print(f\"🚀 Processing batch {batch_num + 1}/{num_batches} ({start_idx + 1}-{end_idx})\")\n",
        "\n",
        "    output = []\n",
        "\n",
        "    for link in tqdm(batch_links, desc=f\"Batch {batch_num + 1}\", unit=\"pdf\"):\n",
        "        pdf_name = Path(link).name\n",
        "        pdf_path = pdf_dir / pdf_name\n",
        "\n",
        "        # Step 1: Download if not already present\n",
        "        if not pdf_path.exists():\n",
        "            os.system(f\"curl -s -L -o '{pdf_path}' '{link}'\")\n",
        "\n",
        "        # Step 2: Extract text\n",
        "        try:\n",
        "            text = smart_extract(pdf_path)\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] {pdf_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Step 3: Structure\n",
        "        try:\n",
        "            data = structure_legal_text(text)\n",
        "            data[\"file_name\"] = pdf_name\n",
        "            data[\"pdf_link\"] = link\n",
        "            output.append(data)\n",
        "        except Exception as e:\n",
        "            print(f\"[Error structuring] {pdf_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Step 4: Save this batch’s output\n",
        "    batch_output = pd.DataFrame(output)\n",
        "    output_file = output_dir / f'structured_output_batch_{batch_num + 1}.csv'\n",
        "    batch_output.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"✅ Batch {batch_num + 1} done → saved to {output_file}\\n\")\n",
        "\n",
        "print(\"🎯 All batches processed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1aEwaqpoVx9",
        "outputId": "02639e72-ef50-403a-9dc3-db31138f7339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total PDFs: 5530 → 56 batches of 100 each\n",
            "\n",
            "🚀 Processing batch 1/56 (1-100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batch 1:   3%|▎         | 3/100 [00:00<00:05, 18.26pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Error] direct_pdf_link: no such file: 'pdfs/direct_pdf_link'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batch 1: 100%|██████████| 100/100 [02:11<00:00,  1.32s/pdf]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Error] repealedfileopen?rfilename=A1838-30.pdf: Cannot open empty file: filename='pdfs/repealedfileopen?rfilename=A1838-30.pdf'.\n",
            "✅ Batch 1 done → saved to outputs/structured_output_batch_1.csv\n",
            "\n",
            "🚀 Processing batch 2/56 (101-200)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batch 2: 100%|██████████| 100/100 [05:22<00:00,  3.22s/pdf]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Batch 2 done → saved to outputs/structured_output_batch_2.csv\n",
            "\n",
            "🚀 Processing batch 3/56 (201-300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batch 3: 100%|██████████| 100/100 [07:55<00:00,  4.76s/pdf]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Batch 3 done → saved to outputs/structured_output_batch_3.csv\n",
            "\n",
            "🚀 Processing batch 4/56 (301-400)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batch 4:  64%|██████▍   | 64/100 [05:06<07:23, 12.31s/pdf]"
          ]
        }
      ]
    }
  ]
}